 \stepcounter{lecture}
 \setcounter{lecture}{2}
 \sektion{Lecture 2}

 Let $A\in \MM_{n,m}(R)$. If $R$ is a field, the rank of $A$ is the dimension of the
 image of $A:R^m\to R^n$, and $m-rk(A)$ is the dimension of the null space. That
 is, whenever $rk(A)< m$, there is a solution to the system of linear equations
 \begin{equation}
 0 = A\cdot x \label{lec02ast}
 \end{equation}
 which says that the columns $\alpha_i\in R^n$ of $A$ satisfy the dependence $\sum
 x_i\alpha_i=0$. The following theorem of McCoy generalizes this so that $R$ can be any
 non-zero commutative ring.
 \begin{theorem}[McCoy]\label{lec02T:McCoy3}
   If $R$ is not the zero ring, the following are equivalent:
   \begin{enumerate}
     \item The columns $\alpha_1$, \dots, $\alpha_m$ are linearly dependent.
     \item Equation \ref{lec02ast} has a nontrivial solution.
     \item $rk(A)<m$.
   \end{enumerate}
 \end{theorem}
 \begin{corollary}
   If $R\ne 0$, the following hold
   \begin{enumerate}
     \item[(a)] If $n<m$ (i.e.\ if there are ``more variables than equations''), then
      Equation \ref{lec02ast} has a nontrivial solution.
     \item[(b)] $R$ has the ``strong rank property'':
        $R^m\hookrightarrow R^n \Longrightarrow m\le n$.
     \item[(c)] $R$ has the ``rank property'':
        $R^n\twoheadrightarrow R^m \Longrightarrow m\le n$.
     \item[(d)] $R$ has the ``invariant basis property'':
        $R^m\cong R^n \Longrightarrow m=n$.
   \end{enumerate}
 \end{corollary}
 \begin{proof}[Proof of Corollary]
   $(a)$ If $n<m$, then $rk(A)\le \min\{n,m\} =n< m$, so by Theorem \ref{lec02T:McCoy3},
   Equation \ref{lec02ast} has a non-trivial solution.

   $(a\Rightarrow b)$ If $m>n$, then by $(a)$, any $R$-linear map $R^m\to R^n$
   has a kernel. Thus, $R^m\hookrightarrow R^n$ implies $m\le n$.

   $(b\Rightarrow c)$ If $R^n\twoheadrightarrow R^m$, then since $R^m$ is free,
   there is a section $R^m\hookrightarrow R^n$ (which must be injective), so $m\le n$.

   $(c\Rightarrow d)$ If $R^m\cong R^n$, then we have surjections both ways, so
   $m\le n\le m$, so $m=n$.
 \end{proof}
 \begin{corollary}
   Let $R\ne 0$, and $A$ some $n\times n$ matrix. Then the following are equivalent
   (1) $\det A\in \C(R)$; (2) the columns of $A$ are linearly independent; (3) the rows of
   $A$ are linearly independent.
 \end{corollary}
 \begin{proof}
   The columns are linearly independent if and only if Equation \ref{lec02ast} has no
   non-trivial solutions, which occurs if and only if the rank of $A$ is equal to $n$,
   which occurs if and only if $\det A$ is a non-zero-divisor.

   The transpose argument shows that $\det A\in \C(R)$ if and only if the rows are
   independent.
 \end{proof}
 \begin{proof}[Proof of the Theorem]
   $0=Ax = \sum \alpha_i x_i$ if and only if the $\alpha_i$ are dependent, so $(1)$ and
   $(2)$ are equivalent.

   $(2\Rightarrow 3)$ Let $x\in R^m$ be a non-zero solution to $A\cdot x=0$. If $n<m$,
   then $rk(A)\le n <m$ and we're done. Otherwise, let $B$ be any $m\times m$ minor of
   $A$ (so $B$ has as many columns as $A$, but perhaps is missing some rows). Then
   $Bx=0$; multiplying by the adjoint of $B$, we get $(\det B)x=0$, so each $x_i$
   annihilates $\det B$. Since $x\neq 0$, some $x_i$ is non-zero, and we have shown that
   $x_i\cdot \D_m(A)=0$, so $rk(A)<m$.

   $(3\Rightarrow 2)$ Assume $r=rk(A)<m$. We may assume $r< n$ (adding a row of
   zeros to $A$ if needed). Fix a nonzero element $a$ such that $a\cdot \D_{r+1}(A)=0$.
   If $r=0$, then take $x$ to be the vector with an $a$ in each place. Otherwise, there
   is some $r\times r$ minor not annihilated by $a$. We may assume it is the upper left
   $r\times r$ minor. Let $B$ be the upper left $(r+1)\times (r+1)$ minor, and let $d_1$,
   \dots, $d_{r+1}$ be the cofactors along the $(r+1)$-th row. We claim that the column
   vector $x = (ad_1,\dots, ad_{r+1},0,\dots, 0)$ is a solution to Equation
   \ref{lec02ast} (note that it is non-zero because $ad_{r+1}\neq 0$ by assumption). To
   check this, consider the product of $x$ with the $i$-th row, $(a_{i1},\dots, a_{im})$.
   This will be equal to $a$ times the determinant of $B'$, the matrix $B$ with the
   $(r+1)$-th row replaced by the $i$-th row of $A$. If $i\le r$, the determinant of $B'$
   is zero because it has two repeated rows. If $i> r$, then $B'$ is an $(r+1)\times
   (r+1)$ minor of $A$, so its determinant is annihilated by $a$.
 \end{proof}
 \begin{corollary}
   Suppose a module ${}_RM$ over a non-zero ring $R$ is generated by $\beta_1,\dots,
   \beta_n\in M$. If $M$ contains $n$ linearly independent vectors, $\gamma_1,\dots,
   \gamma_n$, then the $\beta_i$ form a free basis.
 \end{corollary}
 \begin{proof}
   Since the $\beta_i$ generate, we have $\gamma = \beta\cdot A$ for some $n\times n$
   matrix $A$. If $Ax=0$ for some non-zero $x$, then $\gamma \cdot x = \beta Ax = 0$,
   contradicting independence of the $\gamma_i$. By Theorem \ref{lec02T:McCoy3},
   $rk(A)=n$, so $d=\det(A)$ is a regular element.

   Over $R[d^{-1}]$, there is an inverse $B$ to $A$. If $\beta\cdot
   y=0$ for some $y\in R^n$, then $\gamma By = \beta y=0$. But the $\gamma_i$ remain
   independent over $R[d^{-1}]$ since we can clear the denominators of any linear
   dependence to get a dependence over $R$ (this is where we use that $d\in \C(R)$), so
   $By=0$. But then $y=A\cdot 0 = 0$. Therefore, the $\beta_i$ are linearly independent,
   so they are a free basis for $M$.
%
%
%   We need to show that the $\beta_i$ are linearly independent. Since the $\beta_i$
%   generate, we can write $\gamma = \beta\cdot A$ for some $n\times n$ matrix $A$. We
%   claim that the system $A\cdot x=0$ has no non-trivial solutions. If $Ax=0$, then
%   $\gamma\cdot x = \beta Ax =0$, so we must have $x=0$ since the $\gamma_i$ are
%   independent. By the Theorem, $d=\det A$ is a regular element.
%
%   Now localize! Consider $R\to R[d^{-1}]$. Since $d$ is regular, this is an injection.
%   We also get $M\hookrightarrow M[d^{-1}]$. Locally, there is an inverse $B$ to $A$. To
%   show the independence of the $\beta_i$ (over $M$), assume $\beta\cdot y=0$ for some
%   column vector $y\in R^n$. We have $\gamma \cdot B\cdot y = \beta\cdot y = 0$. The
%   $\gamma_i$ remain independent in the localized module, so $By=0$, so $y = A\cdot 0
%   =0$.
 \end{proof}
