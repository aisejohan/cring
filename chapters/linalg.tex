\chapter{Linear algebra over rings}

\begin{definition}
   An ideal $I\subset R$ is called \emph{dense}\index{dense ideal} if $rI=0$ implies $r=0$.
   This is denoted $I\subseteq_d R$. This is the same as saying that ${}_RI$ is a
   faithful module over $R$.
 \end{definition}
 If $I$ is a principal ideal, say $Rb$, then $I$ is dense exactly when $b\in \mathcal{C}(R)$. The
 easiest case is when $R$ is a domain, in which case an ideal is dense exactly when it is
 non-zero.

 If $R$ is an integral domain, then by working over the quotient field, one can define
 the rank of a matrix with entries in $R$. But if $R$ is not a domain, rank becomes
 tricky. Let $\mathcal{D}_i(A)$ be the $i$-th \emph{determinantal ideal} in $R$, generated by all
 the determinants of $i\times i$ minors of $A$. We define $\mathcal{D}_0(A)=R$. If $i\ge
 \min\{n,m\}$, define $\mathcal{D}_i(A)=(0)$.

 Note that $\mathcal{D}_{i+1}(A)\supseteq \mathcal{D}_i(A)$ because you can expand by minors, so we have a
 chain
 \[
    R=\mathcal{D}_0(A)\supseteq \mathcal{D}_1(A)\supseteq \cdots \supseteq (0).
 \]
 \begin{definition}
   Over a non-zero ring $R$, the \emph{McCoy rank} (or just \emph{rank}) of $A$ to be
   the maximum $i$ such that $\mathcal{D}_i(A)$ is dense in $R$. The rank of $A$ is denoted
   $rk(A)$.
 \end{definition}
 If $R$ is an integral domain, then $rk(A)$ is just the usual rank. Note that over any
 ring, $rk(A)\le \min\{n,m\}$.

 If $rk(A)=0$, then $\mathcal{D}_1(A)$ fails to be dense, so there is some non-zero element $r$
 such that $rA=0$. That is, $r$ zero-divides all of the entries of $A$.

 If $A\in \mathbb{M}_{n,n}(R)$, then $A$ has rank $n$ (full rank) if and only if $\det A$ is a
 regular element.

 \begin{exercise}
   Let $R=\mathbb{Z}/6\mathbb{Z} $, and let $A=diag(0,2,4)$, $diag(1,2,4)$, $diag(1,2,3)$, $diag(1,5,5)$
   ($3\times 3$ matrices). Compute the rank of $A$ in each case.
 \end{exercise}
 \begin{solution}\raisebox{-2\baselineskip}{
   $\begin{array}{c|cccc}
   A & \mathcal{D}_1(A) & \mathcal{D}_2(A) & \mathcal{D}_3(A) & \\ \hline
   diag(0,2,4) & (2) & (2) & (0) & 3\cdot (2)=0\text{, so }rk=0 \\
   diag(1,2,4) & R & (2) & (2) & 3\cdot (2)=0\text{, so }rk=1 \\
   diag(1,2,3) & R & R & (2) & 3\cdot (2)=0\text{, so }rk=2 \\
   diag(1,5,5) & R & R & R & \text{so }rk=3
  \end{array}$}
 \end{solution}
 \section{Lecture 2}

 Let $A\in \mathbb{M}_{n,m}(R)$. If $R$ is a field, the rank of $A$ is the dimension of the
 image of $A:R^m\to R^n$, and $m-rk(A)$ is the dimension of the null space. That
 is, whenever $rk(A)< m$, there is a solution to the system of linear equations
 \begin{equation}
 0 = A\cdot x \label{lec02ast}
 \end{equation}
 which says that the columns $\alpha_i\in R^n$ of $A$ satisfy the dependence $\sum
 x_i\alpha_i=0$. The following theorem of McCoy generalizes this so that $R$ can be any
 non-zero commutative ring.
 \begin{theorem}[McCoy]\label{lec02T:McCoy3}
   If $R$ is not the zero ring, the following are equivalent:
   \begin{enumerate}
     \item The columns $\alpha_1$, \dots, $\alpha_m$ are linearly dependent.
     \item Equation \ref{lec02ast} has a nontrivial solution.
     \item $rk(A)<m$.
   \end{enumerate}
 \end{theorem}
 \begin{corollary}
   If $R\ne 0$, the following hold
   \begin{enumerate}
     \item[(a)] If $n<m$ (i.e.\ if there are ``more variables than equations''), then
      Equation \ref{lec02ast} has a nontrivial solution.
     \item[(b)] $R$ has the ``strong rank property'':
        $R^m\hookrightarrow R^n \Longrightarrow m\le n$.
     \item[(c)] $R$ has the ``rank property'':
        $R^n\twoheadrightarrow R^m \Longrightarrow m\le n$.
     \item[(d)] $R$ has the ``invariant basis property'':
        $R^m\cong R^n \Longrightarrow m=n$.
   \end{enumerate}
 \end{corollary}
 \begin{proof}[Proof of Corollary]
   $(a)$ If $n<m$, then $rk(A)\le \min\{n,m\} =n< m$, so by Theorem \ref{lec02T:McCoy3},
   Equation \ref{lec02ast} has a non-trivial solution.

   $(a\Rightarrow b)$ If $m>n$, then by $(a)$, any $R$-linear map $R^m\to R^n$
   has a kernel. Thus, $R^m\hookrightarrow R^n$ implies $m\le n$.

   $(b\Rightarrow c)$ If $R^n\twoheadrightarrow R^m$, then since $R^m$ is free,
   there is a section $R^m\hookrightarrow R^n$ (which must be injective), so $m\le n$.

   $(c\Rightarrow d)$ If $R^m\cong R^n$, then we have surjections both ways, so
   $m\le n\le m$, so $m=n$.
 \end{proof}
 \begin{corollary}
   Let $R\ne 0$, and $A$ some $n\times n$ matrix. Then the following are equivalent
   (1) $\det A\in \mathcal{C}(R)$; (2) the columns of $A$ are linearly independent; (3) the rows of
   $A$ are linearly independent.
 \end{corollary}
 \begin{proof}
   The columns are linearly independent if and only if Equation \ref{lec02ast} has no
   non-trivial solutions, which occurs if and only if the rank of $A$ is equal to $n$,
   which occurs if and only if $\det A$ is a non-zero-divisor.

   The transpose argument shows that $\det A\in \mathcal{C}(R)$ if and only if the rows are
   independent.
 \end{proof}
 \begin{proof}[Proof of the Theorem]
   $0=Ax = \sum \alpha_i x_i$ if and only if the $\alpha_i$ are dependent, so $(1)$ and
   $(2)$ are equivalent.

   $(2\Rightarrow 3)$ Let $x\in R^m$ be a non-zero solution to $A\cdot x=0$. If $n<m$,
   then $rk(A)\le n <m$ and we're done. Otherwise, let $B$ be any $m\times m$ minor of
   $A$ (so $B$ has as many columns as $A$, but perhaps is missing some rows). Then
   $Bx=0$; multiplying by the adjoint of $B$, we get $(\det B)x=0$, so each $x_i$
   annihilates $\det B$. Since $x\neq 0$, some $x_i$ is non-zero, and we have shown that
   $x_i\cdot \mathcal{D}_m(A)=0$, so $rk(A)<m$.

   $(3\Rightarrow 2)$ Assume $r=rk(A)<m$. We may assume $r< n$ (adding a row of
   zeros to $A$ if needed). Fix a nonzero element $a$ such that $a\cdot \mathcal{D}_{r+1}(A)=0$.
   If $r=0$, then take $x$ to be the vector with an $a$ in each place. Otherwise, there
   is some $r\times r$ minor not annihilated by $a$. We may assume it is the upper left
   $r\times r$ minor. Let $B$ be the upper left $(r+1)\times (r+1)$ minor, and let $d_1$,
   \dots, $d_{r+1}$ be the cofactors along the $(r+1)$-th row. We claim that the column
   vector $x = (ad_1,\dots, ad_{r+1},0,\dots, 0)$ is a solution to Equation
   \ref{lec02ast} (note that it is non-zero because $ad_{r+1}\neq 0$ by assumption). To
   check this, consider the product of $x$ with the $i$-th row, $(a_{i1},\dots, a_{im})$.
   This will be equal to $a$ times the determinant of $B'$, the matrix $B$ with the
   $(r+1)$-th row replaced by the $i$-th row of $A$. If $i\le r$, the determinant of $B'$
   is zero because it has two repeated rows. If $i> r$, then $B'$ is an $(r+1)\times
   (r+1)$ minor of $A$, so its determinant is annihilated by $a$.
 \end{proof}
 \begin{corollary}
   Suppose a module ${}_RM$ over a non-zero ring $R$ is generated by $\beta_1,\dots,
   \beta_n\in M$. If $M$ contains $n$ linearly independent vectors, $\gamma_1,\dots,
   \gamma_n$, then the $\beta_i$ form a free basis.
 \end{corollary}
 \begin{proof}
   Since the $\beta_i$ generate, we have $\gamma = \beta\cdot A$ for some $n\times n$
   matrix $A$. If $Ax=0$ for some non-zero $x$, then $\gamma \cdot x = \beta Ax = 0$,
   contradicting independence of the $\gamma_i$. By Theorem \ref{lec02T:McCoy3},
   $rk(A)=n$, so $d=\det(A)$ is a regular element.

   Over $R[d^{-1}]$, there is an inverse $B$ to $A$. If $\beta\cdot
   y=0$ for some $y\in R^n$, then $\gamma By = \beta y=0$. But the $\gamma_i$ remain
   independent over $R[d^{-1}]$ since we can clear the denominators of any linear
   dependence to get a dependence over $R$ (this is where we use that $d\in \mathcal{C}(R)$), so
   $By=0$. But then $y=A\cdot 0 = 0$. Therefore, the $\beta_i$ are linearly independent,
   so they are a free basis for $M$.
\end{proof}
