
\chapter{Foundations}
\label{foundations}


The present foundational chapter will introduce the notion of a ring and,
next, that of  a module over a ring. These notions will be the focus of the
present book.

We begin with a few historical remarks.  Fermat's last theorem states that the
equation
  \[ \label{ft} x^n  + y^n = z^n \]
has no nontrivial solutions in the integers, for $n \ge 3$.  We could try to
prove this by factoring the expression on the left hand side. We can write
  \[ (x+y)(x+ \zeta y) (x+ \zeta^2y) \dots (x+ \zeta^{n-1}y) = z^n, \]
where $\zeta$ is a primitive $n$th root of unity.  Unfortunately, the factors
lie in $\mathbb{Z}[\zeta]$, not the ring of integers $\mathbb{Z}$.  Though
$\mathbb{Z}[\zeta]$ is still a ring where we have notions of primes and
factorization, just as in $\mathbb{Z}$, we will see that prime factorization
is not always unique in $\mathbb{Z}[\zeta]$.

For instance, consider the ring
$\mathbb{Z}[\sqrt{-5}]$ of complex numbers of the form $a + b\sqrt{-5}$, where
$a, b \in \mathbb{Z}$.  Then we have the two factorizations
  \[ 6 = 2 \cdot 3 = (1 + \sqrt{-5})(1 - \sqrt{-5}). \]
Both of these are factorizations of 6 into irreducible factors, but they
are fundamentally different.

In part, commutative algebra grew out of the need to understand this failure
of unique factorization more generally.
The most natural context for this is that of a \emph{ring}, which we now
introduce. 

\section{Commutative rings and their ideals}

\subsection{Rings}
\begin{definition} 
A \textbf{commutative ring} is a set $R$ with an addition map
$+ : R \times R \to R$ and a multiplication map $\times : R \times R \to R$
that satisfy the following conditions.

\begin{enumerate}
  \item $R$ is a group under addition.
  \item The multiplication map is commutative and distributes over addition.
  This means that $x \times (y+z) = x \times y + x\times z$ and $x \times y = y
  \times x$.
  \item There is a \textbf{unit} (or \textbf{identity element}), denoted by
        $1$, such that $1 \times x = x$ for all $x \in R$.
\end{enumerate}
We shall typically write $xy$ for $x \times y$.

Given a ring, a \textbf{subring} is a subset that contains the identity
element and is closed under addition and multiplication.
\end{definition} 

\begin{example} 
$\mathbb{Z}$ is the simplest example of a ring.
\end{example} 

\begin{exercise}\label{polynomial} Let $R$ be a commutative ring.
Show that the set of polynomials in one variable over $R$ is a commutative
ring $R[x]$. Give a rigorous definition of this.
\end{exercise} 

\begin{example}
For any ring $R$, we can consider the polynomial ring $R[x_1, \ldots, x_n]$
which consists of the polynomials in $n$ variables with coefficients in $R$.
\end{example}


\begin{exercise} 
If $R$ is a commutative ring, recall that an \textbf{invertible element} (or, somewhat
confusingly, a \textbf{unit}) $u \in R$ is an element such
that there exists $v \in R$ with $uv = 1$. Prove that $v$ is necessarily
unique.
\end{exercise}

\subsection{The category of rings}
The class of rings forms a category. Its morphisms are called ring homomorphisms.


\begin{definition}
A \textbf{ring homomorphism} between two rings $R$ and $S$ as a map
$f : R \to S$ that respects addition and multiplication. That is,

\begin{enumerate}
  \item $f(1_R) = 1_S$, where $1_R$ and $1_S$ are the respective identity
        elements.
  \item $f(a + b) = f(a) + f(b)$ for $a, b \in R$.
  \item $f(ab) = f(a)f(b)$ for $a, b \in R$.
\end{enumerate}
There is thus a \emph{category} $\mathbf{Ring}$ whose objects are commutative
rings and whose morphisms are ring-homomorphisms.
\end{definition}

The philosophy of Grothendieck, as expounded in his EGA \cite{EGA}, is that one should
always do things in a relative context. This means that instead of working
with objects, one should work with \emph{morphisms} of objects. Motivated by
this, we introduce:

\begin{definition} 
Given a ring $A$, an \textbf{$A$-algebra} is a ring $R$ together with a
morphism of rings (a \textbf{structure morphism}) $A \to R$. There is a category of $A$-algebras, where a
morphism between $A$-algebras is required to commute with the structure
morphisms. 
\end{definition} 

So if $R$ is an $A$-algebra, then $R$ is not only a ring, but there is a way
to multiply elements of $R$ by elements of $A$ (namely, to multiply $a \in A$
with $r \in R$, take the image of $a $ in $R$, and multiply that by $r$).
For instance, any ring is an algebra over any subring.

If $B$ is an $A$-algebra and $C$ a $B$-algebra, then $C$ is an $A$-algebra in a
natural way. Namely, by assumption we are given morphisms of rings $A \to B$
and $B \to C$, so composing them gives the structure morphism $A \to C$ of $C$
as an $A$-algebra. 


\begin{example} 
Every ring is a $\mathbb{Z}$-algebra in a natural and unique way. There is a
unique map (of rings) $\mathbb{Z} \to R$ for any ring $R$ because a
ring-homomorphism is required to preserve the identity. 
\end{example}

\begin{example} 
If $R$ is a ring, the polynomial ring $R[x]$ is an $R$-algebra in a natural
manner.
\end{example} 

\begin{example} 
$\mathbb{C}$ is an $\mathbb{R}$-algebra.
\end{example} 


After this series of technical definitions, we shall give several more  examples.
\begin{exercise} 
If $R$ is a ring and $G$ a commutative monoid,\footnote{That is, there is a
commutative multiplication on $G$ with an identity element, but not
necessarily with inverses.} then the set
$R[G]$ of formal finite sums $\sum r_i g_i$ with $r_i \in R, g_i \in G$ is a
commutative ring, called the \textbf{group ring}. The case of $G =
\mathbb{Z}_{\geq 0}$ is the polynomial ring. 
\end{exercise} 

\begin{exercise}
\label{integersinitial}
The ring $\mathbb{Z}$ is an \emph{initial object} in the category of rings.
That is, for any ring $R$, there is a \emph{unique} morphism of rings
$\mathbb{Z} \to R$.
\end{exercise} 

\begin{exercise} 
The ring where $0=1$ (the \textbf{zero ring}) is a \emph{final object} in the category of rings. That
is, every ring admits a unique map to the zero ring.	
\end{exercise} 

\begin{exercise} 
Let $X$ be a set and $R$ a ring. The set $R^X$ of functions $f:X \to R$ is a
ring. If $S \subset X$, then the set of functions $f: X \to R$ that vanish on
$S$ is an ideal.
\end{exercise}

\begin{exercise}
\label{corepresentable}
Let $\mathcal{C}$ be a category and $F: \mathcal{C} \to \mathbf{Sets}$  a
covariant functor. Recall that $F$ is said to be \textbf{corepresentable} if
$F$ is naturally isomorphic to $X \to \hom_{\mathcal{C}}(U, X)$ for some
object $U \in \mathcal{C}$. For instance, the functor sending everything to a
one-point set is corepresentable if and only if $\mathcal{C}$ admits an
initial object.

Prove that the functor  $\mathbf{Rings} \to \mathbf{Sets}$ assigning to each ring its underlying set is
representable. (Hint: use a suitable polynomial ring.)
\end{exercise} 


\subsection{Ideals}

An \emph{ideal} in a ring is  analogous to a normal subgroup of a
group. As we shall see, one may quotient by ideals just as one quotients by
normal subgroups. 

\begin{definition}
Let $R$ be a ring.  An \textbf{ideal} in $R$ is a subset $I \subset R$ that
satisfies the following.

\begin{enumerate}
  \item $0 \in I$.
  \item If $x, y \in I$, then $x + y \in I$.
  \item If $x \in I$ and $y \in R$, then $xy \in I$.
\end{enumerate}
\end{definition}

There is a simple way of obtaining ideals, which we now describe.
Given elements $x_1, \ldots, x_n \in R$, we denote by $(x_1, \ldots, x_n) \subset
R$ the subset of linear combinations $\sum r_i x_i$, where $r_i \in R$.  This
is clearly an ideal, and in fact the smallest one containing all $x_i$.  It is
called the ideal \textbf{generated} by $x_1, \ldots, x_n$.  A
\textbf{principal ideal} $(x)$ is one generated by a single $x \in R$.

\begin{example}
Ideals generalize the notion of divisibility.  Note that
in $\mathbb{Z}$, the set of elements divisible by $n \in \mathbb{Z}$ forms the
ideal $I = n\mathbb{Z} = (n)$.
\end{example}

\begin{exercise} 
Show that the ideal $(2, 1 + \sqrt{-5}) \subset \mathbb{Z}[\sqrt{-5}]$ is not
principal.
\end{exercise}

\subsection{Operations on ideals}

There are a number of simple operations that one may do with ideals, which we
now describe.

\begin{definition}
The sum $I + J$ of two ideals $I, J \subset R$ is defined as the set of sums
  \[ \left\{ x + y : x \in I, y \in J \right\}. \]
\end{definition}

\begin{definition}
The product $IJ$ of two ideals $I, J \subset R$ is defined as the smallest
ideal containing the products $xy$ for all $x \in I, y \in J$. This is just
the set
  \[ \left\{ \sum x_i y_i : x_i \in I, y_i \in J \right\}. \]
\end{definition}

We leave the basic verification of properties as an exercise:
\begin{exercise}
Given ideals $I, J \subset R$, verify the following.

\begin{enumerate}
  \item $I + J$ is the smallest ideal containing $I$ and $J$.
  \item  $IJ$ is contained in $I$ and $J$.
  \item $I \cap J$ is an ideal.
\end{enumerate}
\end{exercise}

\begin{example}
In $\mathbb{Z}$, we have the following for any $m, n$.

\begin{enumerate}
  \item $(m) + (n) = (\gcd\{ m, n \})$,
  \item $(m)(n) = (mn)$,
  \item $(m) \cap (n) = (\mathrm{lcm}\{ m, n \})$.
\end{enumerate}
\end{example}

\begin{proposition}
For ideals $I, J, K \subset R$, we have the following.

\begin{enumerate}
  \item Distributivity: $I(J + K) = IJ + IK$.
  \item $I \cap (J + K) = I \cap J + I \cap K$ if $I \supseteq J$ or $I \supseteq K$.
  \item If $I + J = R$, $I \cap J = IJ$.
\end{enumerate}

\begin{proof}
1 and 2 are clear.  For 3, note that $(I + J)(I \cap J) = I(I \cap J)
+ J(I \cap J) \subseteq IJ$.  Since $IJ \subseteq I \cap J$, the result
follows.
\end{proof}
\end{proposition}

\subsection{Zerodivisors}


Let $R$ be a commutative ring.
\begin{definition} 
If $r \in R$, then $r$ is called  a \textbf{zerodivisor} if there is $s \in R, s
\neq 0$ with $sr = 0$. Otherwise $r$ is called a \textbf{nonzerodivisor.}
\end{definition} 

As an example, we prove a basic result on the zerodivisors in a polynomial ring.

\begin{proposition}
Let $A=R[x]$. Let $f=a_nx^n+\cdots +a_0\in A$. If there is a non-zero polynomial $g\in
A$ such that $fg=0$, then there exists $r\in R\smallsetminus\{0\}$ such that $f\cdot
 r=0$.
\end{proposition}
So all the coefficients are zerodivisors.
\begin{proof}
 Choose $g$ to be of minimal degree, with leading coefficient $bx^d$. We may assume
 that  $d>0$. Then $f\cdot b\neq 0$, lest we contradict minimality of $g$. We must have
$a_i g\neq 0$ for some $i$. To see this, assume that $a_i\cdot g=0$, then $a_ib=0$ for
all $i$ and then $fb=0$. Now pick $j$ to be the largest integer such that $a_jg\neq
   0$. Then $0=fg=(a_0 + a_1x + \cdots a_jx^j)g$, and looking at the leading coefficient,
   we get $a_jb=0$. So $\deg (a_jg)<d$. But then $f\cdot (a_jg)=0$, contradicting
   minimality of $g$.
 \end{proof}

\begin{exercise} 
The product of two nonzerodivisors is a nonzerodivisor, and the product of two
zerodivisors is a zerodivisor. It is, however, not necessarily true that the
\emph{sum} of two zerodivisors is a zerodivisor.
\end{exercise} 

\subsection{Quotient rings}

We next describe a procedure for producing new rings from old ones. 
If $R$ is a ring and $I \subset R$ an ideal, then the quotient group $R/I$
is a ring in its own right. If $a+I, b+I$ are two cosets, then the
multiplication is $(a+I)(b+I) = ab + I$.  It is easy to check that this does
not depend on the coset representatives $a,b$.

As one easily checks, this becomes to a multiplication 
\[ R/I \times R/I \to R/I  \] 
which is commutative and associative, and 
whose identity element is $1+I$.
In particular, $R/I$ is a ring, under multiplication $(a+I)(b+I) = ab+I$. 
\begin{definition} 
$R/I$ is called the \textbf{quotient ring} by the ideal $I$.
\end{definition} 

The
reduction map $\phi \colon R \to R/I$ is a ring-homomorphism with a
\emph{universal
property}.
Namely, for any ring $B$, there is a map
 \[ \hom(R/I, B) \to \hom(R, B)  \]
 on the hom-sets
 by composing with the ring-homomorphism $\phi$; this map is injective and the
 image consists of all homomorphisms $R \to B$ which vanish on $I$.  
Stated alternatively, to map out of $R/I$ (into some ring $B$) is the same thing as mapping out of
$R$ while killing the ideal $I \subset R$.

This is best thought out for oneself, but here is the detailed justification.
The reason is that any map $R/I \to B$ pulls back to a map $R \to R/I \to B$
which annihilates $I$ since $R \to R/I$ annihilates $I$. Conversely, if we have
a map 
\[ f: R \to B  \]
killing $I$, then we can define $R/I \to B$ by sending $a+I$ to $f(a)$; this is
uniquely defined since $f$ annihilates $I$.

\begin{exercise} 
 If $R$ is a commutative
ring, an element $e \in R$ is said to be \textbf{idempotent} if $e^2 =
e$. Define a covariant functor $\mathbf{Rings} \to \mathbf{Sets}$ sending a
ring to its idempotents. Prove that it is corepresentable. (Answer: the
corepresenting object is $\mathbb{Z}[X]/(X - X^2)$.)
\end{exercise} 

\begin{exercise} 
Show that the functor assigning to each ring the set of elements annihilated
by 2 is corepresentable. 
\end{exercise} 

\begin{exercise}
If $I \subset J \subset R$, then $J/I$ is an ideal of $R/I$, and there is a
canonical isomorphism
\[ (R/I)/(J/I) \simeq R/J.  \]
\end{exercise} 



\section{Further examples}

We now illustrate a few important examples of 
commutative rings. The section is in large measure an advertisement for why
one might care about commutative algebra; nonetheless, the reader is
encouraged at least to skim this section.

\subsection{Rings of holomorphic functions}

The following subsection may be omitted without impairing understanding.

There is a fruitful analogy in number theory between the rings $\mathbb{Z}$ and
$\mathbb{C}[t]$, the latter being the polynomial ring over $\mathbb{C}$ in one
variable (\rref{polynomial}).  Why are they analogous? Both of these rings have a theory of unique
factorization:  that is, factorization into primes or irreducible polynomials. (In the
latter, the irreducible polynomials have degree one.)
Indeed we know:
\begin{enumerate}
\item Any nonzero integer factors as a product of primes (possibly times $-1$). 
\item Any  nonzero polynomial factors as a product of an element of
$\mathbb{C}^* =\mathbb{C} - \left\{0\right\}$ and polynomials of the form $t -
a, a \in \mathbb{C}$.
\end{enumerate}


There is another way of thinking of $\mathbb{C}[t]$ in terms of complex
analysis.  This is equal to the ring of holomorphic functions on $\mathbb{C}$
which are meromorphic at infinity.  
Alternatively, consider the Riemann sphere $\mathbb{C} \cup \{ \infty\}$; then the ring $\mathbb{C}[t]$
consists of meromorphic functions on the sphere whose poles (if any) are at
$\infty$. 

This description admits generalizations. 
Let $X$ be a
Riemann surface.  (Example: take the complex numbers modulo a lattice, i.e. an
elliptic curve.)
Suppose that $x \in X$. Define $R_x$ to be the ring of meromorphic functions on $X$
which are allowed poles only at $x$ (so are everywhere else holomorphic). 

\begin{example} Fix the notations of the previous discussion. 
Fix $y \neq x \in X$. Let $R_x$ be the ring of meromorphic functions on the
Riemann surface $X$ which are holomorphic on $X - \left\{x\right\}$, as before.
Then the collection of functions that vanish at $y$ forms an
\emph{ideal} in $R_x$. 

There are lots of other ideals. For instance, fix two
points $y_0, y_1 \neq x$; we look at the ideal of $R_x$ that vanish at both $y_0, y_1$.

\end{example} 


\textbf{For any Riemann surface $X$, the conclusion of Dedekind's theorem
(\rref{ded1}) applies.  } In other
words, the ring  $R_x$ as defined in the example admits  unique factorization of
ideals. We shall call such rings \textbf{Dedekind domains} in the future.

\begin{example} Keep the preceding notation.

Let $f \in R_x$, nonzero. By definition, $f$ may have a pole at $x$, but no poles elsewhere. $f$ vanishes
at finitely many points $y_1, \dots, y_m$. When $X$ was the Riemann sphere,
knowing the zeros of $f$ told us something about $f$. Indeed, in this case
$f$ is just a
polynomial, and we have a nice factorization of $f$ into functions in $R_x$ that vanish only
at one point. In general Riemann surfaces, this
is not generally possible.  This failure turns out to be very interesting.

Let $X = \mathbb{C}/\Lambda$ be an elliptic curve (for $\Lambda \subset
\mathbb{C}^2$ a lattice), and suppose $x = 0$. Suppose we
are given $y_1, y_2, \dots, y_m \in X$ that are nonzero; we ask whether there
exists a function $f \in R_x$ having simple zeros at $y_1, \dots, y_m$ and nowhere else.
The answer is interesting, and turns out to recover the group structure on the
lattice.

\begin{proposition} 
A function $f \in R_x$ with simple zeros only at the $\left\{y_i\right\}$ exists if and only if $y_1 + y_2 + \dots + y_n = 0$ (modulo $\Lambda$).

\end{proposition} 
So this problem of finding a function with specified zeros is equivalent to
checking that the specific zeros add up to zero with the group structure.

In any case, there might not be such a nice function, but we have at least an
ideal $I$ of functions that have zeros (not necessarily simple) at $y_1, \dots,
y_n$.  This ideal has unique factorization into the ideals of functions
vanishing at $y_1$, functions vanishing at $y_2$, so on.  
\end{example} 

\subsection{Ideals and varieties}

We saw in the previous subsection that ideals can be thought of as the
vanishing of functions. This, like divisibility, is another interpretation,
which is particularly interesting in algebraic geometry.


Recall the  ring $\mathbb{C}[t]$ of complex polynomials discussed in the
last subsection. More generally, if $R$ is a ring,  we saw in
\rref{polynomial} that the set $R[t]$ of polynomials with coefficients
in $R$
is a ring.  This is a construction that
can be iterated to get a polynomial ring in several variables over $R$.

\begin{example} 
Consider the polynomial ring $\mathbb{C}[x_1, \dots, x_n]$. Recall that before
we thought of the ring $\mathbb{C}[t]$ as a ring of meromorphic functions.  
Similarly each element of the polynomial ring $\mathbb{C}[x_1, \dots, x_n]$
gives a function $\mathbb{C}^n \to \mathbb{C}$; we can think of the polynomial
ring as sitting inside the ring of all functions $\mathbb{C}^n \to \mathbb{C}$.

A question you might ask: What are the ideals in this ring?  One way to get an
ideal is to pick a point $x=(x_1, \dots, x_n) \in \mathbb{C}^n$; consider the
collection of all functions $f \in \mathbb{C}[x_1, \dots, x_n]$ which vanish on
$x$; by the usual argument, this is an ideal.

There are, of course, other ideals. More generally, if $Y \subset
\mathbb{C}^n$, consider the collection of polynomial functions $f:
\mathbb{C}^n \to \mathbb{C}$ such that $f \equiv 0$ on
$Y$.  This is easily seen to be an ideal in the polynomial ring. We thus have a
way of taking a subset of $\mathbb{C}^n$ and producing an ideal.
Let $I_Y$ be the ideal corresponding to $Y$.  

This construction is not injective. One can have $Y \neq Y'$ but $I_Y = I_{Y'}$. For instance, if $Y$ is dense in
$\mathbb{C}^n$, then $I_Y = (0)$, because the only way a continuous function on
$\mathbb{C}^n$ can vanish on $Y$ is for it to be zero.

There is a much closer connection in the other direction. You might ask whether
all ideals can arise in this way. The quick answer is no---not even when $n=1$. The ideal $(x^2) \subset \mathbb{C}[x]$ cannot be obtained
in this way.  It is easy to see that the only way we could get this as $I_Y$ is
for $Y=\left\{0\right\}$, but $I_Y$ in this case is just $(x)$, not $(x^2)$.
What's going wrong in this example is that $(x^2)$ is not a \emph{radical}
ideal.
\end{example} 

\begin{definition}\label{def-radical-ideal} 
An ideal $I \subset R$ is \textbf{radical} if whenever $x^2 \in I$, then $x \in
I$.
\end{definition} 

The ideals $I_Y$ in the polynomial ring are all radical.  This is obvious.
You might now ask whether this is the only obstruction. We now state a theorem
that we will prove later.

\begin{theorem}[Hilbert's Nullstellensatz] If $I \subset \mathbb{C}[x_1, \dots,
x_n]$ is a radical ideal, then $I = I_Y$ for some $Y \subset \mathbb{C}^n$. In
fact, the canonical choice of $Y$ is the set of points where all the functions
in $Y$ vanish.\footnote{Such a subset is called an algebraic variety.}
\end{theorem} 


This will be one of the highlights of the present course. But before we can
get to it, there is much to do.

\begin{exercise} 
Assuming the Nullstellensatz, show that any \emph{maximal} ideal in the
polynomial ring $\mathbb{C}[x_1, \dots, x_n]$ is of the form 
$(x_1-a_1, \dots, x_n-a_n)$ for $a_1, \dots, a_n \in \mathbb{C}$. An ideal of a
ring is called \textbf{maximal} if the only ideal that contains it is the
whole ring (and it itself is not the whole ring).

As a corollary, deduce that if $I \subset \mathbb{C}[x_1, \dots, x_n]$ is a
proper ideal (an ideal is called \textbf{proper} if it is not equal to the
entire ring), then there exists $(x_1, \dots, x_n) \in \mathbb{C}^n$ such that
every polynomial in $I$ vanishes on the point $(x_1, \dots, x_n)$. This is
called the \textbf{weak Nullstellensatz.}
\end{exercise} 

\section{Modules over a commutative ring}



We will now establish some basic terminology about modules.

\subsection{Definitions}
Suppose $R$ is a commutative ring.  

\begin{definition} 
An \textbf{$R$-module $M$} is an abelian group $M$ with a map $R \times M \to
M$ (written $(a,m) \to am$) such that
\begin{enumerate}[\textbf{M} 1]
\item  $(ab) m = a(bm)$ for $a,b \in R, m \in M$, i.e. there is an associative law. 
\item $1m
= m$; the unit acts as the identity. 
\item There are distributive laws
on both sides:
$(a+b)m = am + bm$ and $a(m+n) = am + an$ for $a,b \in R, \ m,n \in M$.

\end{enumerate} \end{definition} 

Another definition can be given  as follows.
\begin{definition} 
If $M$ is an abelian group, $End(M)$ is the set of homomorphisms $f: M \to M$.  
This can be made into a (noncommutative) \emph{ring}.\footnote{A
noncommutative ring is one satisfying all the usual axioms of a ring except
that multiplication is not required to be commutative.} Addition is defined pointwise, and
multiplication is by composition. The identity element is the identity
function $1_M$.
\end{definition} 

We made the following definition earlier for commutative rings, but for
clarity we re-state it:
\begin{definition} 
If $R, R'$ are rings (possibly noncommutative) then a function $f: R \to R'$ is a
\textbf{ring-homomorphism}  or \textbf{morphism} if it is compatible with the
ring structures, i.e
\begin{enumerate}
\item  $f(x+y) = f(x) + f(y)$
\item $f(xy) = f(x)f(y)$
\item  $f(1) = 1$.
\end{enumerate}
\end{definition} 

The last condition is not redundant because otherwise the zero map would
automatically be a homomorphism.
The alternative definition of a module is left to the reader in the following
exercise.
\begin{exercise}
If $R$ is a ring and $R \to End(M)$ a homomorphism, then $M$ is made into an
$R$-module, and vice versa.  
\end{exercise}


\begin{example} 
if $R$ is a ring, then $R$ is an $R$-module by multiplication on the left. 
\end{example} 
\begin{example} 
A $\mathbb{Z}$-module is the same thing as an abelian group.
\end{example} 

\begin{definition} 
If $M$ is an $R$-module, a subset $M_0 \subset M$ is a \textbf{submodule} if it
is a subgroup (closed under addition and inversion) and is closed under
multiplication by elements of $R$, i.e. $aM_0 \subset M_0$ for $a \in R$. A
submodule is a module in its own right. If $M_0 \subset M$ is a submodule,
there is a commutative diagram:
\[ \xymatrix{
R \times M_0 \ar[d] \ar[r] &  M_0 \ar[d] \\ R \times M \ar[r] &  M
}.\]
Here the horizontal maps are multiplication.
\end{definition} 

\begin{example} 
Let $R$ be a (\textbf{commutative}) ring; then an ideal in $R$ is the same thing as a
submodule of $R$.
\end{example} 

\begin{example} 
If $A$ is a ring, an $A$-algebra is an $A$-module in an obvious way. More
generally, if $A$ is a ring and $R$ is an $A$-algebra, any $R$-module becomes
an $A$-module by pulling back the multiplication map via $A \to R$. 
\end{example} 



Dual to submodules  is the notion of a \emph{quotient module}, which we define
next:
\begin{definition} Suppose $M$ is an $R$-module and $M_0$  a
submodule.  Then the abelian group $M/M_0$ (of cosets)  is an $R$-module,
called the \textbf{quotient module} by $M_0$.  

Multiplication is as follows. If
one has a coset $x  + M_0 \in M/M_0$, one  multiplies this by $a \in R$ to
get the coset $ax
+ M_0$. This does not depend on the coset representative.  
\end{definition} 


\subsection{The categorical structure on modules}
So far, we have talked about modules, but we have not discussed morphisms
between modules, and have yet to make the class of modules over a given ring
into a category. This we do next.

Let us thus  introduce a  few more basic notions.

\begin{definition} 
Let $R$ be a ring.  Suppose $M,N$ are $R$-modules.  A map $f: M \to N$
is a \textbf{module-homomorphism} if it preserves all the relevant structures.
Namely, it must be a homomorphism of abelian groups, $f(x+y) = f(x) + f(y)$,
and second it must
preserve multiplication:
$$f(ax)  = af(x)$$ for $a \in R, x \in M$. 
\end{definition}

A simple way of getting plenty of module-homomorphisms is simply to consider
multiplication by a fixed element of the ring. 
\begin{example} 
If $a \in R$, then multiplication by $a$ is a module-homomorphism $M
\stackrel{a}{\to} M$ for any $R$-module $M$.\footnote{When one considers
modules over noncommutative rings, this is no longer true.} Such homomorphisms
are called \textbf{homotheties.}
\end{example} 


If $M \stackrel{f}{\to} N$ and $N \stackrel{g}{\to} P$ are
module-homomorphisms, their composite $M \stackrel{g \circ f}{\to} P$ clearly
is too. 
Thus, for any commutative ring $R$, the class of $R$-modules and
module-homomorphisms forms a \textbf{category}.

\begin{exercise} 
The initial object in this category is the zero module, and this is also the
final object.

In general, a category where the initial object and final object are the same
(that is, isomorphic) is called a \emph{pointed category.} The common object
is called the \emph{zero object.} In a pointed category $\mathcal{C}$, there is a morphism
$X \to Y$ for any two objects $X, Y \in \mathcal{C}$: if $\ast$ is the zero
object, then we can take $X \to \ast \to Y$. This is well-defined and is
called the \emph{zero morphism.}
One can easily show that the composition (on the left or the right) of a
zero morphism is a zero morphism (between a possibly different set of objects). 

In the case of the category of modules, the zero object is clearly the zero
module, and the zero morphism $M \to N$ sends $m \mapsto 0$ for each $m \in M$.
\end{exercise} 
\begin{definition} Let $f: M \to N$ be a module homomorphism.
In this case, the \textbf{kernel} $\ker f$ of $f$ is  the set of elements $m
\in M$ with $f(m)=0$. This is
a submodule of $M$, as is easy to see. 

The \textbf{image} $\im f$ of $f$ (the set-theoretic
image, i.e. the collection of all $f(x), x \in M$) is also a submodule of $N$. 

The
\textbf{cokernel} of $f$ is defined by
\(  N/\im(f).  \) 
\end{definition}

\begin{exercise} \label{univpropertykernel} 
The universal property of the kernel is as follows. Let $M \stackrel{f}{\to }
N$ be a morphism with kernel $K \subset M$. Let $T \to M$ be a map. Then $T \to M$ factors through the
kernel $K \to M$ if and only if its composition with $f$ (a morphism $T \to N$) is zero. 
That is, an arrow $T \to K$ exists in the diagram (where the dotted arrow
indicates we are looking for a map that need not exist)
\[ \xymatrix{
& T \ar@{-->}[ld] \ar[d]  \\
K \ar[r] &  M \ar[r]^f &  N
}\]
if and only if the composite $T \to N$ is zero.
In particular, if we think of the hom-sets as abelian groups (i.e.
$\mathbb{Z}$-modules)
\[ \hom_R( T,K) = \ker\left( \hom_R(T, M) \to \hom_R(T, N) \right). \]
\end{exercise}

In other words, one may think of the kernel as follows. If $X
\stackrel{f}{\to} Y$ is a morphism, then the kernel $\ker(f)$ is the equalizer
of $f$ and the zero morphism $X \stackrel{0}{\to} Y$.

\begin{exercise} 
What is the universal property of the cokernel?
\end{exercise} 

\begin{exercise} \label{moduleunderlyingsetrepresentable}
On the category of modules, the functor assigning to each module $M$ its
underlying set is corepresentable (cf. \rref{corepresentable}). What
is the corepresenting object? 
\end{exercise} 

We shall now introduce the notions of \emph{direct sum} and \emph{direct
product}. Let $I$ be a set, and suppose that for each $i \in I$, we are given
an $R$-module $M_i$.

\begin{definition} 
The \textbf{direct product} $\prod M_i$ is set-theoretically the cartesian product. It is given
the structure of an $R$-module by addition and multiplication pointwise on
each factor. 
\end{definition} 
\begin{definition} 
The \textbf{direct sum} $\bigoplus_I M_i$ is the set of elements in the direct
product such that all but finitely many entries are zero. The direct sum is a
submodule of the direct product.
\end{definition} 


\begin{exercise} \label{productcoproduct}
The direct product is a product in the category of modules, and the direct sum
is a coproduct.
\end{exercise} 

\cref{productcoproduct} shows that the category of modules over a fixed
commutative ring has products and coproducts. In fact, the category of modules
is both complete and cocomplete (\cref{completecat}).
To see this, it suffices to show that (by
\cref{coprodcoequalsufficeforcocomplete} and its dual) that this category
admits equalizers and coequalizers.

The equalizer of two maps
\[ M \stackrel{f,g}{\rightrightarrows} N  \]
is easily checked to be the submodule of $M$ consisting of $m \in M$ such that
$f(m) = g(m)$, or, in other words, the kernel of $f-g$. The coequalizer of these two maps is the quotient module of $N$
by the submodule $\left\{f(m) - g(m), m \in M\right\}$, or, in other words,
the cokernel of $f-g$.

Thus:

\begin{proposition} 
If $R$ is a ring, the category of $R$-modules is complete and cocomplete.
\end{proposition} 

\subsection{Exactness}
Finally, we introduce the notion of \emph{exactness}. 
\begin{definition} \label{exactness}
Let $f: M \to N$ be a morphism of $R$-modules.  Suppose $g: N \to P$ is another morphism of
$R$-modules.  

The pair of maps is a \textbf{complex} if $g \circ f = 0: M \to N \to P$.
This is equivalent to the condition that $\im(f) \subset \ker(g)$. 

This complex is \textbf{exact} (or exact at $N$) if $\im(f) = \ker(g)$.
In other words, anything that is killed when mapped to $P$ actually comes from something in
$M$. 

\end{definition} 


We shall often write pairs of maps as sequences
\[ A \stackrel{f}{\to} B \stackrel{g}{\to} C  \]
and say that the sequence is exact if the pair of maps is, as in
\rref{exactness}. A longer (possibly infinite) sequence of modules
\[ A_0 \to A_1 \to A_2 \to \dots  \]
will be called a \textbf{complex} if each set of three
consecutive terms is a complex, and \textbf{exact} if it is exact at each step.

\begin{example} 
The sequence $0 \to A \stackrel{f}{\to} B$ is exact if and only if the map $f$
is injective. Similarly, $A \stackrel{f}{\to} B \to 0$ is exact if and only if
$f$ is surjective. Thus, $0 \to A \stackrel{f}{\to}  B \to 0$ is exact if and
only if $f$ is an isomorphism.
\end{example} 

One typically sees this definition applied to sequences of the form
\[ 0 \to M'\stackrel{f}{ \to} M \stackrel{g}{\to} M'' \to 0,  \]
which, if exact, is called a \textbf{short exact sequence}. 
Exactness here means that $f$ is injective, $g$ is surjective, and $f$ maps
onto the kernel of $g$.  So $M''$ can be thought of as the quotient $M/M'$.

\begin{example} 
Conversely, if $M$ is a module and $M' \subset M$ a submodule, then there is a
short exact sequence
\[ 0 \to M' \to M \to M/M' \to 0.  \]
So every short exact sequence is of this form.
\end{example} 


Suppose   $F$ is a functor from the category of $R$-modules to the
category of  $S$-modules, where $R, S$ are rings.  Then:

\begin{definition} 
\begin{enumerate}
\item  $F$ is called \textbf{additive} if $F$ preserves direct sums.  
\item  $F$ is called \textbf{exact} if $F$ is additive and preserves exact sequences.  
\item  $F$ is called \textbf{left exact} if $F$ is additive and preserves exact sequences of the form
$0 \to M' \to M \to M''$.  Equivalently, $F$ preserves kernels.  
\item  $F$ is \textbf{right exact} if $F$ is additive and $F$ preserves exact
sequences of the form $M' \to M \to M'' \to 0$, i.e. $F$ preserves cokernels.  
\end{enumerate}
\end{definition} 

The reader should note that the above definition can be recast using the more
general notion of an \emph{abelian category,}  which axiomatizes much of the
standard properties of the category of modules over a ring. Such a
generalization turns out to be necessary when many natural categories, such as
the category of chain complexes or the category of sheaves on a topological
space, are not naturally categories of modules.
We do not go into this here, cf. \cite{Ma98}. 

A functor  $F$	is exact if and only if it is both left and right exact.  
This actually requires proof, though it is not hard. Namely, right-exactness implies that $F$
preserves cokernels. Left-exactness implies that $F$ preserves kernels. $F$
thus preserves images, as the image of a morphism is the kernel of its cokernel.
So if
\[ A \to B \to C  \]
is a short exact sequence, then the kernel of the second map is equal to the
image of the first; we have just seen that this is preserved under $F$.


\begin{exercise} 
Suppose whenever $0 \to A' \to A \to A'' \to 0$ is short exact, then $FA' \to
FA \to FA'' \to 0$ is exact. Prove that $F$ is right-exact. So we get a
slightly weaker criterion for right-exactness.

Do the same for left-exact functors.
\end{exercise} 

\subsection{Split exact sequences}

Let $f: A \to B$ be a map of sets which is injective. Then there is a map $g: A
\to B$ such that the composite $g \circ f: A \stackrel{f}{\to} B
\stackrel{g}{\to} A$ is the identity. Namely, we define $g$ to be the inverse
of $f$ on $f(A)$ and arbitrarily on $B-f(A)$.
Conversely, if $f: A \to B$ admits an element $g: B \to A$ such that $g \circ f
= 1_A$, then $f$ is injective. This is easy to see, as any $a \in A$ can be
``recovered'' from $f(a)$ (by applying $g$).

In general, however, this observation does not generalize to arbitrary
categories.

\begin{definition} 
Let $\mathcal{C}$ be a category. A morphism $A \stackrel{f}{\to} B$ is called a
\textbf{split injection} if there is $g: B \to A$ with $g \circ f = 1_A$.
\end{definition} 

\begin{exercise}[General nonsense]
Suppose $f: A \to B$ is a split injection. Show that $f$ is a categorical monomorphism.
(Idea: the map $\hom(C,A) \to \hom(C,B)$ becomes a split injection of sets
thanks to $g$.)
\end{exercise} 

\add{what is a categorical monomorphism? Maybe omit the exercise}

In the category of sets, we have seen above that \emph{any} monomorphism is a
split injection. This is not true in other categories, in general.

\begin{exercise} 
Consider the morphism $\mathbb{Z} \to \mathbb{Z}$ given by multiplication by
2. Show that this is not a split injection: no left inverse $g$ can exist.
\end{exercise} 

We are most interested in the case of modules over a ring.

\begin{proposition} 
A morphism $f: A \to B$ in the category of $R$-modules is a split injection if
and only if:
\begin{enumerate}
\item $f$ is injective. 
\item $f(A)$ is a direct summand in $B$.
\end{enumerate}
\end{proposition} 
The second condition means that there is a submodule $B' \subset B$ such that
$B = B' \oplus f(A)$ (internal direct sum). In other words, $B = B'  + f(A)$
and $B' \cap f(A) = \left\{0\right\}$.
\begin{proof} 
Suppose the two conditions hold, and we have a module $B'$ which is a
complement to $f(A)$.
Then we define a left inverse
\[ B \stackrel{g}{\to} A  \]
by letting $g|_{f(A)} = f^{-1}$ (note that $f$ becomes an \emph{isomorphism}
$A \to f(A)$) and $g|_{B'}=0$. It is easy to see that this is indeed a left
inverse, though in general not a right inverse, as $g$ is likely to be
non-injective.

Conversely, suppose $f: A \to B$ admits a left inverse $g: B \to A$. The usual
argument (as for sets) shows that $f$ is injective. The essentially new
observation is that $f(A) $ is a direct summand in $B$. To define the
complement, we take $\ker(g) \subset B$.
It is easy to see (as $g \circ f = 1_A$) that $\ker(g) \cap f(A) =
\left\{0\right\}$. Moreover, $\ker(g) +f(A)$ fills $B$: given $b \in B$, it is
easy to check that
\[ b - f(g(b)) \in \ker(g).  \]
Thus we find that the two conditions are satisfied.
\end{proof} 




\add{further explanation, exactness of filtered colimits}


\subsection{The five lemma}

The five lemma will be a useful tool for us in proving that maps are
isomorphisms. Often this argument is used in inductive proofs. Namely, we will
see that often ``long exact sequences'' (extending infinitely in one or both
directions) arise from short exact sequences in a natural way. In such
events, the five lemma
will allow us to prove that certain morphisms are isomorphisms by induction on
the dimension. 
\begin{theorem} 
Suppose given a commutative diagram
\[ \xymatrix{
A \ar[d] \ar[r] &  B \ar[d] \ar[r] &  C \ar[d]  \ar[r] &  D \ar[d] \ar[r] & E \ar[d]  \\
A' \ar[r] &  B' \ar[r] &  C' \ar[r] &  D' \ar[r] &  E'
}\]
such that the rows are exact and the four vertical maps $A \to A', B \to B', D
\to D', E \to E'$ are isomorphisms. Then $C \to C'$ is an isomorphism.
\end{theorem}

This is the type of proof that goes by the name of ``diagram-chasing,'' and
is best thought out visually for oneself, even though we give a complete proof. 

\begin{proof} 
We have the diagram
\[ 
\xymatrix{
A \ar[r]^k \ar[d]^\a & B \ar[r]^l \ar[d]^\b 
	& C \ar[r]^m \ar[d]^g & D \ar[r]^n \ar[d]^\d & E \ar[d]^\e  \\
F \ar[r]_p & G \ar[r]_q & H \ar[r]_r & I \ar[r]_s & J 
}
\] 
where the rows are exact at $B, C, D, G, H, I$ and the squares commute. In
addition, suppose that $\a, \b, \d, \e$ are isomorphisms. We will show that
$\g$ is an isomorphism.

\emph{We show that $\g$ is surjective:}

Suppose that $h \in H$. Since $\d$ is surjective, there exists an element 
$d \in D$ such that $r(h) = \d(d) \in I$.
By the commutativity of the rightmost square, $s(r(h)) = \e(n(d))$. 
The exactness at $I$ means that $\im r = \ker s$, so hence
$\e(n(d)) = s(r(h)) = 0$. Because $\e$ is injective, $n(d) = 0$.
Then $d \in \ker(n) = \im(m)$ by exactness at $D$.
Therefore, there is some $c \in C$ such that $m(c) = d$.
Now, $\d(m(c)) = \d(d) = r(h)$ and by the commutativity of squares, 
$\d(m(c)) = r(\g(c))$, so therefore $r(\g(c)) = r(h)$. Since $r$ is a
homomorphism, $r(\g(c) - h) = 0$. Hence $\g(c) - h \in \ker r = \im q$ by
exactness at $H$.

Therefore, there exists $g \in G$ such that $q(g) = \g(c) - h$.
$\b$ is surjective, so there is some $b \in B$ such that $\b(b) = g$ and hence
$q(\b(b)) = \g(c) - h$. By the commutativity of squares, 
$q(\b(b)) = \g(l(b)) = \g(c) - h$. Hence 
$h = \g(c) - \g(l(b)) = \g(c - l(b))$, and therefore $\g$ is surjective.

So far, we've used that $\b$ and $\g$ are surjective, $\e$ is injective, and
exactness at $D$, $H$, $I$.

\emph{We show that $\g$ is injective:}

Suppose that $c \in C$ and $\g(c) = 0$.
Then $r(\g(c)) = 0$, and by the commutativity of squares, 
$\d(m(c)) = 0$. Since $\d$ is injective, $m(c) = 0$, so
$c \in \ker m = \im l$ by exactness at $C$. 
Therefore, there is $b \in B$ such that $l(b) = c$.
Then $\g(l(b)) = \g(c) = 0$, and by the commutativity of squares, 
$q(\b(b)) = 0$. Therefore, $\b(b) \in \ker q$, and by exactness at $G$, 
$\b(b) \in \ker q = \im p$.

There is now $f \in F$ such that $p(f) = \b(b)$. Since $\a$ is surjective, this
means that there is $a \in A$ such that $f = \a(a)$, so then 
$\b(b) = p(\a(a))$. By commutativity of squares, 
$\b(b) = p(\a(a)) = \b(k(a))$, and hence $\b(k(a) - b) = 0$.
Since $\b$ is injective, we have $k(a) -b = 0$, so $k(a) = b$.
Hence $b \in \im k = \ker l$ by commutativity of squares, so $l(b) = 0$.
However, we defined $b$ to satisfy $l(b) = c$, so therefore $c = 0$ and hence
$\g$ is injective.

Here, we used that $\a$ is surjective, $\b, \d$ are injective, and exactness at
$B, C, G$.

Putting the two statements together, we see that $\g$ is both surjective and
injective, so $\g$ is an isomorphism. We only used that $\b, \d$ are
isomorphisms and that $\a$ is surjective, $\e$ is injective, so we can slightly
weaken the hypotheses; injectivity of $\a$ and surjectivity of $\e$ were
unnecessary.

\end{proof} 


\section{Ideals}

The notion of an \emph{ideal} has already been defined. Now we will introduce additional terminology related to the theory of ideals.

\subsection{Prime and maximal ideals}

Recall that the notion of an ideal generalizes that of divisibility. In
elementary number theory, though, one finds that questions of divisibility
basically reduce to questions about primes. 
The notion of a ``prime ideal'' is intended to generalize the familiar idea of a prime
number.

\begin{definition} 
An ideal $I \subset R$ is said to be \textbf{prime} if
\begin{enumerate}[\textbf{P} 1]
\item  $1 \notin I$ (by convention, 1 is not a prime number)
\item If $xy \in I$, either $x \in I$ or $y \in I$.
\end{enumerate}
\end{definition} 

\begin{example}
\label{integerprimes}
If $R = \mathbb{Z}$ and $p \in R$, then $(p) \subset \mathbb{Z}$ is a prime ideal iff $p$ or $-p$ is a
prime number in $\mathbb{N}$ or if $p$ is zero. 
\end{example} 



If $R$ is any commutative ring, there are two obvious ideals. These obvious
ones are the zero ideal $(0)$
consisting only of the zero element, and the unit element $(1)$ consisting of all of
$R$.


\begin{definition} \label{maximalideal}
An ideal $I \subset R$ is called \textbf{maximal}\footnote{Maximal with
respect to not being the unit ideal.} if 
\begin{enumerate}[\textbf{M} 1]
\item  $1 \notin I$
\item Any larger ideal contains $1$ (i.e., is all of $R$).
\end{enumerate}
\end{definition} 

So a maximal ideal is a maximal element in the partially ordered set of proper
ideals (an ideal is \textbf{proper} if it does not contain 1).

\begin{exercise} 
Find the maximal ideals in $\mathbb{C}[t]$.
\end{exercise} 


\begin{proposition} 
A maximal ideal is prime.
\end{proposition} 
\begin{proof} 
First, a maximal ideal does not contain 1.

Let $I \subset R$ be a maximal ideal.
We need to show that if $xy \in I$,
then one of $x,y \in I$.  If $x \notin I$, then $(I,x) = I + (x)$ (the ideal
generated by $I$ and $x$) strictly contains $I$, so by maximality contains
$1$.  In particular, $1 \in I+(x)$, so we can write
\[ 1 = a + xb  \]
where $a \in I, b \in R$. Multiply both sides by $y$:
\[ y = ay  + bxy.  \]
Both terms on the right here are in $I$ ($a \in I$ and $xy \in I$), so we find
that $y \in I$.

\end{proof} 

Given a ring $R$, what can we say about the collection of ideals in $R$?
There
are two obvious ideals in $R$, namely $(0)$ and $ (1)$.  These are the same if and
only if $0=1$, i.e. $R$ is the zero ring.
So for any nonzero commutative ring, we have at least two distinct ideals.  

Next, we show that maximal ideals always \emph{do} exist, except in the case
of the zero ring. 
\begin{proposition} \label{anycontainedinmaximal}
Let $R$ be a commutative ring. Let $I \subset R$ be a proper ideal.  Then $I$
is contained in a maximal ideal.
\end{proposition} 

\begin{proof} 
This requires the axiom of choice in the form of Zorn's lemma.  Let
$P$ be the collection of all ideals $J \subset R$ such that $I
\subset J$ and $J \neq R$.  Then $P$ is a poset with respect to  inclusion.  $P$ is
nonempty because it contains $I$.  Note that given a (nonempty) linearly ordered
collection of ideals $J_{\alpha} \in P$, the union $\bigcup J_{\alpha} \subset
R$ is an ideal: this is easily seen in view of the linear ordering (if $x,y
\in \bigcup J_{\alpha}$, then both $x,y$ belong to some $J_{\gamma}$, so $x+y
\in J_{\gamma}$; multiplicative closure is even easier). The union is not all
of $R$ because it does not contain $1$.  

This implies that $P$ has a maximal element by Zorn's lemma.  This maximal element may
be called $\mathfrak{M}$; it's a proper element containing $I$. I claim that
$\mathfrak{M}$ is a maximal ideal, because if it were contained in a larger
 ideal, that would  be in $P$ (which cannot happen by maximality) unless it were all of $R$.
\end{proof} 

\begin{corollary} 
Let $R $ be a nonzero commutative ring.  Then $R$ has a maximal ideal.
\end{corollary} 
\begin{proof} 
Apply the lemma to the zero ideal.  
\end{proof} 

\begin{corollary} 
Let $R$ be a nonzero commutative ring. Then $x \in R$ is invertible if and
only if it belongs to no maximal ideal $\mathfrak{m} \subset R$.
\end{corollary} 
\begin{proof} 
Indeed, $x$ is invertible if and only if $(x) = 1$. That is, if and only if
$(x)$ is not a proper ideal; now \rref{anycontainedinmaximal}
finishes the argument.
\end{proof} 

\subsection{Fields and integral domains}

Recall:

\begin{definition} 
A commutative ring $R$ is called a  \textbf{field} if $1 \neq 0$ and for every $x \in R -
\left\{0\right\}$ there exists an \textbf{inverse} $x^{-1} \in R$ such that $xx^{-1} =
1$.


\end{definition}


This condition has an obvious interpretation in terms of ideals.
\begin{proposition} 
A commutative ring with $1 \neq 0$ is a field iff it has only the two ideals $(1),
(0)$.
\end{proposition} 

Alternatively, a ring is a field if and only if $(0)$ is a maximal ideal.

\begin{proof} 
Assume $R$ is a field.  Suppose $I \subset R$.  If $I \neq (0)$, then there is
a nonzero $x \in I$. Then there is an inverse $x^{-1}$. We have $x^{-1} x =1
\in I$, so $I = (1)$.
In a field, there is thus 	no room for ideals other than $(0)$ and $(1)$.

To prove the converse, assume every ideal of $R$ is $(0)$ or $(1)$. Then for
each $x \in R$, $(x) = (0)$ or $(1)$. If $x \neq 0$, the first cannot happen, so
that means that the ideal generated by $x$ is the unit ideal. So $1$ is a
multiple of $x$, implying that $x$ has a multiplicative inverse.
\end{proof} 

So fields also have an uninteresting ideal structure.

\begin{corollary} \label{maximalfield}
If $R$ is a ring and $I \subset R$ is an ideal, then $I$ is maximal if and only
if $R/I$ is a field.
\end{corollary} 

\begin{proof}
The basic point here is that there is a bijection between the ideals of $R/I$
and ideals of $R$ containing $I$. 

Denote  by $\phi: R \to R/I$ the reduction map. There is a
construction mapping ideals of $R/I$ to ideals of $R$. This sends an ideal in
$R/I$ to
its inverse image.  This is easily seen to map to ideals of $R$ containing $I$.
The map from ideals of $R/I$ to ideals of $R$ containing $I$ is a bijection,
as one checks easily.

It follows that $R/I$ is a field precisely if
$R/I$ has precisely two ideals, i.e. precisely if there are precisely two
ideals in $R$ containing $I$. These ideals must be $(1)$ and $I$, so this
holds if and only if $I$ is maximal.
\end{proof} 

There is a similar characterization of prime ideals.

\begin{definition} 
A commutative ring $R$ is an \textbf{integral domain} if for all $ x,y \in R$,
$x \neq 0 $ and $y \neq 0$ imply $xy \neq 0$.
\end{definition} 

\begin{proposition} \label{primeifdomain} 
An ideal $I \subset R$ is prime iff $R/I$ is a domain.
\end{proposition} 

\begin{exercise} 
Prove \rref{primeifdomain}.
\end{exercise} 

Any field is an integral domain. This is because in a field, nonzero elements
are invertible, and the product of two invertible elements is invertible. This
statement translates in ring theory to the statement that a maximal ideal is
prime.


Finally, we include an example that describes what \emph{some} of the prime
ideals in a polynomial ring look like.
\begin{example} 
Let $R$ be a ring and $P$ a prime ideal. We claim that $PR[x] \subset R[x]$ is a
prime ideal.

Consider the map $\tilde{\phi}:R[x]\rightarrow(R/P)[x]$ with
$\tilde{\phi}(a_0+\cdots+a_nx^n)=(a_0+P)+\cdots+(a_n+P)x^n$. This is clearly
a homomorphism because $\phi:R\rightarrow R/P$ is, and its kernel consists
of those polynomials $a_0+\cdots+a_nx^n$ with $a_0,\ldots,a_n\in P$, which is
precisely $P[x]$. Thus $R[x]/P[x]\simeq (R/P)[x]$, which is an integral domain
because $R/P$ is an integral domain. Thus $P[x]$ is a prime ideal. 

However, if
$P$ is a maximal ideal, then $P[x]$ is never a maximal ideal because the ideal
$P[x]+(x)$ (the polynomials with constant term in $P$) always strictly contains
$P[x]$ (because if $x\in P[x]$ then $1\in P$, which is impossible). Note
that $P[x]+(x)$ is the kernel of the composition of $\tilde{\phi}$ with
evaluation at 0, i.e $(\text{ev}_0\circ\tilde{\phi}):R[x]\rightarrow R/P$,
and this map is a surjection and $R/P$ is a field, so that $P[x]+(x)$ is
the maximal ideal in $R[x]$ containing $P[x]$.
\end{example} 


\begin{exercise} 
Let $R$ be a domain. Consider the set of formal quotients $a/b, a, b \in R$
with $b \neq 0$. Define addition and multiplication using usual rules. Show
that the resulting object $K(R)$ is a ring, and in fact a \emph{field}. The
natural map $R \to K(R)$, $r \to r/1$, has a universal property. If $R
\hookrightarrow L$ is an injection of $R$ into a field $L$, then there is a
unique morphism $K(R) \to L$ of fields extending $R \to L$. This construction
will be generalized when we consider \emph{localization.}

Note that a non-injective map $R\to L$ will \emph{not} factor through the
quotient field!
\end{exercise} 


\begin{exercise}\label{Jacobson} 
Let $R$ be a commutative ring. Then the \textbf{Jacobson radical} of $R$ is
the intersection $\bigcap \mathfrak{m}$ of all maximal ideals $\mathfrak{m}
\subset R$. Prove that an element $x$ is in the Jacobson radical if and only
if $1 - yx$ is invertible for all $y \in R$.
\end{exercise} 

\subsection{Prime avoidance}
\begin{theorem}[Prime Avoidance] \label{primeavoidance}
   Let $I_1,\dots, I_n \subset R$ be ideals. Let $A\subset R$ be a subset which is closed
   under addition and multiplication. Assume that at least $n-2$ of the ideals are
   prime. If $A\subseteq I_1\cup \cdots \cup I_n$, then $A\subseteq I_j$ for some $j$.
 \end{theorem}
 \begin{proof}
   Induct on $n$. If $n=1$, the result is trivial. The case $n=2$ is an easy argument: if
   $a_1\in A\smallsetminus I_1$ and $a_2\in A\smallsetminus I_2$, then $a_1+a_2\in
   A\smallsetminus (I_1\cup I_2)$.

   Now assume $n\ge 3$. We may assume that for each $j$, $A\not\subseteq I_1\cup \cdots
   \cup \hat I_j\cup \cdots I_n$.\footnote{The hat means omit $I_j$.} Fix an element
   $a_j\in A\smallsetminus (I_1\cup \cdots \cup \hat I_j\cup \cdots I_n)$. Then this
   $a_j$ must be contained in $I_j$ since $A\subseteq \bigcup I_j$. Since $n\ge 3$, one
   of the $I_j$ must be prime. We may assume that $I_1$ is prime. Define
   $x=a_1+a_2a_3\cdots a_n$, which is an element of $A$. Let's show that $x$ avoids
   \emph{all} of the $I_j$. If $x\in I_1$, then $a_2a_3\cdots a_n\in I_1$, which
   contradicts the fact that $a_i\not\in I_j$ for $i\neq j$ and that $I_1$ is prime. If
   $x\in I_j$ for $j\ge 2$. Then $a_1\in I_j$, which contradicts $a_i\not\in I_j$ for
   $i\neq j$.
 \end{proof}
\subsection{The Chinese remainder theorem}

Let $m,n$ be relatively prime integers. Suppose $a, b \in \mathbb{Z}$; then
one can show that the two congruences $x \equiv a \mod m$
and $x \equiv b \mod n$ can be solved simultaneously in $x \in \mathbb{Z}$.
The solution is unique, moreover, modulo $mn$.
The Chinese remainder theorem generalizes this fact:


\begin{theorem}[Chinese remainder theorem] Let $I_1, \dots I_n$ be ideals in a
ring $R$ which satisfy $I_i + I_j = R$ for $i \neq j$. Then we have $I_1 \cap
\dots \cap I_n = I_1 \dots I_n$ and the morphism of rings
\[ R \to \bigoplus R/I_i \]
is an epimorphism with kernel $I_1 \cap \dots \cap I_n$.
\end{theorem} 

\begin{proof} 
First, note that for any two ideals $I_1$ and $I_2$, we
have $I_1I_2\subseteq I_1\cap I_2$ and $(I_1+I_2)(I_1\cap I_2)\subseteq
I_1I_2$ (because any element of $I_1+I_2$ multiplied by any element of
$I_1\cap I_2$ will clearly be a sum of products of elements from both $I_1$
and $I_2$). Thus, if $I_1$ and $I_2$ are coprime, i.e. $I_1+I_2=(1)=R$,
then $(1)(I_1\cap I_2)=(I_1\cap I_2)\subseteq I_1I_2\subseteq I_1\cap I_2$,
so that $I_1\cap I_2=I_1I_2$. This establishes the result for $n=2$.

If the
ideals $I_1,\ldots,I_n$ are pairwise coprime and the result holds for $n-1$,
then $$\bigcap_{i=1}^{n-1} I_i=\prod_{i=1}^{n-1}I_i.$$  Because $I_n+I_i=(1)$
for each $1\leq i\leq n-1$, there must be $x_i\in I_n$ and $y_i\in I_i$ such
that $x_i+y_i=1$. Thus, $z_n=\prod_{i=1}^{n-1}y_i=\prod_{i=1}^{n-1}(1-x_i)\in
\prod_{i=1}^{n-1} I_i$, and clearly $z_n+I_n=1+I_n$ since each $x_i\in
I_n$. Thus $I_n+\prod_{i=1}^{n-1}I_i=I_n+\bigcap_{i=1}^{n-1}I_i=(1)$,
and we can now apply the $n=2$ case to conclude that $\bigcap_{i=1}^n
I_i=\prod_{i=1}^n I_i$. 

Note that for any $i$, we can construct a $z_i$
with $z_i\in I_j$ for $j\neq i$ and $z_i+I_i=1+I_i$ via the same procedure.

 Define $\phi:R\rightarrow\bigoplus R/I_i$
by $\phi(a)=(a+I_1,\ldots,a+I_n)$. The kernel of $\phi$ is
$\bigcap_{i=1}^n I_i$, because $a+I_i=0+I_i$ iff $a\in I_i$, so that
$\phi(a)=(0+I_1,\ldots,0+I_n)$ iff $a\in I_i$ for all $i$, that is,
$a\in\bigcap_{i=1}^n I_i$. Combined with our previous result, the kernel
of $\phi$ is $\prod_{i=1}^n I_i$.

Finally, recall that we constructed
$z_i\in R$ such that $z_i+I_i=1+I_i$, and $z+I_j=0+I_j$ for all $j\neq
i$, so that $\phi(z_i)=(0+I_1,\ldots,1+I_{i},\ldots,0+I_n)$. Thus,
$\phi(a_1z_1+\cdots+a_nz_n)=(a_1+I_1,\ldots,a_n+I_n)$ for all $a_i\in R$,
so that $\phi$ is onto. By the first isomorphism theorem, we have that
$R/I_1\cdots I_n\simeq \bigoplus_{i=1}^nR/I_i$.   \\

\end{proof} 

\section{Some special classes of domains}

\subsection{Principal ideal domains}

\begin{definition} 
A ring $R$ is a \textbf{principal ideal domain} or \textbf{PID} if $R \neq 0$, $R$ is not a
field, $R$ is a domain, and every ideal of $R$ is principal.
\end{definition} 

These have the next simplest theory of ideals.
Each ideal is very simple---it's principal---though there might be a lot of ideals.

\begin{example} 
$\mathbb{Z}$ is a PID. The only nontrivial fact to check here is that:
\begin{proposition} 
Any nonzero ideal $I \subset \mathbb{Z}$ is principal.
\end{proposition} 
\begin{proof} 
If $I = (0)$, then this is obvious.  Else there is $n \in I -
\left\{0\right\}$; we can assume $n>0$.  Choose $n \in I$ as small as possible and
positive. Then  I claim that the ideal $I$ is generated by $(n)$. Indeed, we have $(n)
\subset I$ obviously. If $m \in I$ is another integer, then divide $m$ by $n$,
to find $m = nb + r$ for $r \in [0, n)$. We find that $r \in I$ and $0 \leq r <
n$, so $r=0$, and $m$ is divisible by $n$. And $I \subset (n)$. 

So $I = (n)$.
\end{proof} 
\end{example} 

A module $M$ is said to be \emph{finitely generated} if there exist elements
$x_1, \dots, x_n \in M$ such that any element of $M$ is a linear combination
(with coefficients in $R$) of the $x_i$. (We shall define this more formally
below.)
One reason that PIDs are so convenient is:

\begin{theorem}[Structure theorem] \label{structurePID} 
If $M$ is a finitely generated module over a principal ideal domain $R$, then
$M$ is isomorphic to a direct sum
\[ M \simeq \bigoplus_{i=1}^n R/a_i,  \]
for various $a_i \in R$ (possibly zero).
\end{theorem} 

\add{at some point, the proof should be added. This is important!}

\subsection{Unique factorization domains}

The integers $\mathbb{Z}$ are especially nice because of the fundamental
theorem of arithmetic, which states that every integer has a unique
factorization into primes. This is not true for every integral domain.

\begin{definition} 
An element of a domain $R$ is \textbf{irreducible} if it cannot be written
as the product of two non-unit elements of $R$.
\end{definition} 

\begin{example} 
Consider the integral domain $\mathbb{Z}[\sqrt{-5}]$. We saw earlier that 
\[ 
6 = 2 \cdot 3 = (1 + \sqrt{-5})(1 - \sqrt{-5}),
\] 
which means that $6$ was written as the product of two non-unit elements in
different ways. $\mathbb{Z}[\sqrt{-5}]$ does not have unique factorization.
\end{example} 

\begin{definition} 
A domain $R$ is a \textbf{unique factorization domain} or \textbf{UFD} if every
non-unit $x \in R$ satisfies
\begin{enumerate}
\item $x$ can be written as a product $x = p_1 p_2 \cdots p_n$ of 
irreducible elements $p_i \in R$
\item if $x = q_1 q_2 \cdots q_m$ where $q_i \in R$ are irreducible
then the $p_i$ and $q_i$ are the same up to order and multiplication by units.
\end{enumerate}
\end{definition} 

\begin{example}
$\mathbb{Z}$ is a UFD, while $\mathbb{Z}[\sqrt{-5}]$ is not. In fact, many of
our favorite domains have unique factorization. We will prove that all PIDs 
are UFDs. In particular, in  \rref{gaussianintegersareprincipal} and
\rref{polyringisprincipal}, we saw that $\mathbb{Z}[i]$ and $F[t]$ are PIDs,
so they also have unique factorization.
\end{example}

\begin{theorem} \label{PIDUFD} 
Every principal ideal domain is a unique factorization domain.
\end{theorem} 

\begin{proof} 
Suppose that $R$ is a principal ideal domain and $x$ is an element of $R$. We
first demonstrate that $x$ can be factored into irreducibles.
If $x$ is a unit or an irreducible, then we are done. Therefore, we can assume
that $x$ is reducible, which means that $x = x_1 x_2$ for non-units 
$x_1, x_2 \in R$. If there are irreducible, then we are again done, so we
assume that they are reducible and repeat this process. We need to show that
this process terminates.

Suppose that this process continued infinitely. Then we have an infinite
ascending chain of ideals, where all of the inclusions are proper:
$(x) \subset (x_1) \subset (x_{11}) \subset \cdots \subset R$.
We will show that this is impossible because any infinite ascending chain of
ideals $I_1 \subset I_2 \subset \cdots \subset R$ of a principal ideal domain 
eventually becomes stationary, i.e. for some $n$, $I_k = I_n$ for $k \geq n$.
Indeed, let $I = \bigcup_{i=1}^\infty I_i$. This is an ideal, so it is 
principally generated as $I = (a)$ for some $a$. Since $a \in I$, we must have 
$a \in I_N$ for some $N$, which means that the chain stabilizes after $I_N$.

It remains to prove that this factorization of $x$ is unique. We induct on
the number of irreducible factors $n$ of $x$. If $n = 0$, then $x$ is a unit,
which has unique factorization up to units. Now, suppose that 
$x = p_1 \cdots p_n = q_1 \cdots q_m$ for some $m \ge n$. Since $p_1$ divides
$x$, it must divide the product $q_1 \cdots q_m$ and by irreducibility, one of
the factors $q_i$. Reorder the $q_i$ so that $p_1$ divides $q_1$. However,
$q_1$ is irreducible, so this means that $p_1$ and $q_1$ are the same up to
multiplication by a unit $u$. Canceling $p_1$ from each of the two
factorizations, we see that $p_2 \cdots p_n = u q_2 \cdots q_m = q_2' \cdots
q_m$. By induction, this shows that the factorization of $x$ is unique up to
order and multiplication by units.
\end{proof} 


\subsection{Euclidean domains}

A euclidean domain is a special type of principal ideal domain. In practice,
it will often happen that one has an explicit proof that a given domain is
euclidean, while it might not be so trivial to prove that it is a UFD without
the general implication below.

\begin{definition}
An integral domain $R$ is a \textbf{euclidean domain} if there is a function
$|\cdot |:R\to \mathbb \mathbb{Z}_{\geq 0}$ (called the norm) such that the following hold.
\begin{enumerate}
\item $|a|=0$ iff $a=0$.
\item For any nonzero $a,b\in R$ there exist $q,r\in R$ such that $b=aq+r$ and $|r|<|a|$.
\end{enumerate}
In other words, the norm is compatible with division with remainder.
\end{definition}
\begin{theorem}
A euclidean domain is a principal ideal domain.
\end{theorem}
\begin{proof}
Let $R$ be an euclidean domain, $I\subseteq R$ and ideal, and $b$ be the nonzero element of smallest norm in $I$.
Suppose $ a\in I$. Then we can write $ a = qb + r$ with $ 0\leq r < |b|$, but since $ b$ has minimal nonzero absolute value, $ r = 0$ and $ b|a$. Thus $ I=(b)$ is principal.
\end{proof}


As we will see, this implies that any euclidean domain admits \emph{unique
factorization.}

\begin{proposition} \label{polyringED} 
Let $F$ be a field. Then the polynomial ring $F[t]$ is a euclidean domain. 
In particular, it is a PID.
\end{proposition} 
\begin{proof} 
We define \add{} 
\end{proof} 


\begin{exercise} \label{gaussianintegersareprincipal}
Prove that $\mathbb{Z}[i]$ is principal. 
(Define the norm as $N(a+ib) = a^2 + b^2$.)
\end{exercise} 

\begin{exercise} \label{polyringisprincipal}
Prove that the polynomial ring $F[t]$ for $F$ a field is principal. 
\end{exercise} 


It is \emph{not} true that a PID is necessarily euclidean. Nevertheless, it
was shown in \cite{Gre97} that the converse is ``almost'' true. Namely,
\cite{Gre97} defines the notion of an \textbf{almost euclidean domain.}
A domain $R$ is almost euclidean if there is a function $d: R \to
\mathbb{Z}_{\geq 0}$ such that
\begin{enumerate}
\item $d(a) = 0$ iff $a = 0$. 
\item $d(ab) \geq d(a)$ if $b \neq 0$.
\item  If $a,b \in R - \left\{0\right\}$, then either $b \mid a$ or there is 
$r \in (a,b)$ with $d(r)<d(b)$.
\end{enumerate}

It is easy to see by the same argument that an almost euclidean domain is a PID.
(Indeed, let $R$ be an almost euclidean domain, and $I \subset R$ a nonzero
ideal. Then choose $x \in I - \left\{0\right\}$ such that $d(x)$ is minimal among elements in
$I$. Then if $y \in I - \left\{0\right\}$, either $x \mid y$ or $(x,y) \subset I$ contains an
element with smaller $d$. The latter cannot happen, so the former does.)
However, in fact:
\begin{proposition}[\cite{Gre97}]
A domain is a PID if and only if it is almost euclidean.
\end{proposition} 
\begin{proof} 
Indeed, let $R$ be a PID. Then $R$ is a UFD (\rref{PIDUFD}), so for any $x \in R$,
there is a factorization into prime elements, unique up to units. If $x$
factors into $n$ elements, we define $d(x)=n$; we set $d(0)=0$.
The first two conditions for an almost euclidean domain are then evident. 

Let $x = p_1 \dots p_m$ and $y = q_1 \dots q_n$ be two elements of $R$,
factored into irreducibles. Suppose $x \nmid y$. Choose a generator $b$ of the (principal) ideal $(x,y)$; then obviously $y
\mid b$ so $d(y) \leq d(b)$. But if $d(y) = d(b)$, then the
number of factors of $y$ and $b$ is the same, so  $y \mid b$ would imply
that $y$ and $b$ are associates. This is a contradiction, and implies that
$d(y)<d(b)$.

\end{proof} 


\begin{remark} 
We have thus seen that a euclidean domain is a PID, and a PID is a UFD. Both
converses, however, fail. By Gauss's lemma (\rref{}), the
polynomial ring $\mathbb{Z}[X]$ has unique factorization, though the ideal
$(2, X)$ is not principal. 

In \cite{Ca88}, it is shown that the ring $\mathbb{Z}[\frac{1+
\sqrt{-19}}{2}]$ is a PID but not euclidean (i.e. there is \emph{no} euclidean
norm on it).
\end{remark} 


\section{Basic properties of modules}

\subsection{Free modules}

We now describe a simple way of constructing modules over a ring, and an
important class of modules.

\begin{definition} 
A module $M$ is \textbf{free} if it is isomorphic to $\bigoplus_I R$ for some
index set $I$. The cardinality of $I$ is called the \textbf{rank}.
\end{definition} 

\begin{example} 
$R$ is the simplest example of a free module.
\end{example} 

The claim now is that the notion of ``rank'' is well-defined for a free
module. To see this, we will have to use the notion 
of a \emph{maximal ideal} (\rref{maximalideal}) and
\rref{maximalfield}.
Indeed, suppose
$\bigoplus_I R$ and $\bigoplus_J R$ are isomorphic; we must show that $I$ and
$J$ have the same cardinality. Choose a maximal ideal $\mathfrak{m}
\subset R$. Then, by applying the functor $M \to
M/\mathfrak{m}M$, we find that the $R/\mathfrak{m}$-\emph{vector spaces}
\[ \bigoplus_I R/\mathfrak{m}, \quad \bigoplus_J R/\mathfrak{m}  \]
are isomorphic. By linear algebra, $I$ and $J$ have the same cardinality. 


Free modules have a bunch of nice properties. The first is that it is very
easy to map out of a free module.
\begin{example} 
Let $I$ be an indexing set, and $M$ an $R$-module. Then to give a morphism
\[ \bigoplus_I R \to M  \]
is equivalent to picking an element of $M$ for each $i \in I$. Indeed, given
such a collection of elements $\left\{m_i\right\}$, we send the generator of $\bigoplus_I R$ with a 1
in the $i$th spot and zero elsewhere to $m_i$.
\end{example}

\begin{example} 
In a domain, every principal ideal (other than zero) is a free module of rank
one.
\end{example} 

Another way of saying this is that the free module $\bigoplus_I R$ represents
the functor on modules sending $M$ to the \emph{set} $ M^I$. We have already seen a special case of this for $I$ a
one-element set (\rref{moduleunderlyingsetrepresentable}).

The next claim is that free modules form a reasonably large class of the
category of $R$-modules.

\begin{proposition} \label{freesurjection} 
Given an $R$-module $M$, there is a free module $F$ and a surjection
\[ F \twoheadrightarrow M.  \]
\end{proposition} 
\begin{proof} 
We let $F$ to be the free $R$-module on the elements $e_m$, one for each $m
\in M$. We define the map
\[ F \to M  \]
by describing the image of each of the generators $e_m$: we just send each
$e_m$ to $m \in M$. It is clear that this map is surjective.
\end{proof} 


We close by making a few remarks on matrices.
Let $M$ be a free module of rank $n$, and fix an isomorphism $M \simeq R^n$.
Then we can do linear algebra with $M$, even though we are working over a
ring and not necessarily a field, at least to some extent.
For instance, we can talk about $n$-by-$n$ matrices over the ring $R$, and
then each of them induces a transformation, i.e. a module-homomorphism, $M \to
M$; it is easy to see that every module-homomorphism between free modules is
of this form. Moreover, multiplication of matrices corresponds to composition
of homomorphisms, as usual.
 
\begin{example} Let us consider the question of when the transformation
induced by an $n$-by-$n$ matrix is invertible. The answer is similar to the
familiar one from linear algebra in the case of a field. Namely, the condition
is that the determinant be invertible.

Suppose that an $n \times n$ matrix $A$ over a ring $R$ is invertible. This
means that there exists $A^{-1}$ so that $A A^{-1} = I$, so hence
$1 = \det I = \det(A A^{-1}) = (\det A) (\det A^{-1})$, and therefore, 
$\det A$ must be a unit in $R$.

Suppose instead that an $n \times n$ matrix $A$ over a ring $R$ has an
invertible determinant. Then, using Cramer's rule, we can actually construct
the inverse of $A$.
\end{example} 


\subsection{Finitely generated modules}

The notion of a ``finitely generated'' module is analogous to that of a
finite-dimensional vector space.

\begin{definition} 
An $R$-module $M$ is \textbf{finitely generated} if there exists a surjection
$R^n \to M$ for some $n$. In other words, it has a finite number of elements
whose ``span'' contains $M$.
\end{definition} 

The basic properties of finitely generated modules follow from the fact that
they are stable under extensions and quotients.

\begin{proposition} \label{exact-fingen}
Let $0 \to M' \to M \to M'' \to 0$ be an exact sequence. If $M', M''$ are
finitely generated, so is $M$.
\end{proposition} 
\begin{proof} 
Suppose $0\rightarrow
M'\stackrel{f}{\rightarrow}M\stackrel{g}{\rightarrow}M''\rightarrow0$
is exact. Then $g$ is surjective, $f$ is injective, and
$\text{ker}(g)=\text{im}(f)$. Now suppose $M'$ is finitely genereated,
say by $\{a_1,\ldots,a_s\}$, and $M''$ is finitely generated, say by
$\{b_1,\ldots,b_t\}$. Because $g$ is surjective, each $g^{-1}(b_i)$ is
non-empty. Thus, we can fix some $c_i\in g^{-1}(b_i)$ for each $i$.

For any
$m\in M$, we have $g(m)=r_1b_1+\cdots+r_tb_t$ for some $r_i\in R$ because
$g(m)\in M''$ and $M''$ is generated by the $b_i$. Thus $g(m)=r_1g(c_i)+\cdots
r_tg(c_t)=g(r_1c_1+\cdots+r_tc_t)$, and because $g$ is a homomorphism
we have $m-(r_1c_1+\cdots+r_tc_t)\in\text{ker}(g)=\text{im}(f)$. But
$M'$ is generated by the $a_i$, so the submodule $\text{im}(f)\subset
M$ is finitely generated by the $d_i=f(a_i)$.

Thus, any $m\in
M$ has $m-(r_1c_1+\cdots+r_tc_t)=r_{t+1}d_1+\cdots+r_{t+s}d_s$
for some $r_1,\ldots,r_{t+s}$, thus $M$ is finitely generated by
$c_1,\ldots,c_t,d_1,\ldots,d_s$.  \\

\end{proof} 

The converse is false. It is possible for finitely generated modules to have
submodules which are \emph{not} finitely generated. As we shall see in
\rref{noetherian}, this does not happen over \emph{noetherian} rings.



\begin{example} 
Consider the ring $R=\mathbb{C}[X_1, X_2, \dots,]$ and the ideal $(X_1, X_2,
\dots)$. This ideal is a submodule of the finitely generated $R$-module $R$,
but it is not finitely generated.
\end{example} 

\begin{exercise} 
Show that a quotient of a finitely generated module is finitely generated.
\end{exercise}

\begin{exercise} 
Consider a \emph{split} exact sequence $0 \to M' \to M \to M'' \to 0$. In this
case, show that if $M$ is finitely generated, so is $M'$.
\end{exercise} 


\subsection{Finitely presented modules}

Over messy rings, the notion of a finitely presented module is often a good
substitute for that of a finitely generated one. In fact, we are going to see
(\rref{}), that there is a general method of reducing questions about finitely
presented modules over arbitrary rings to finitely generated modules over
finitely generated $\mathbb{Z}$-algebras.

Throughout, fix a ring $R$.

\begin{definition} 
An $R$-module $M$ is \textbf{finitely presented} if there is an exact sequence
\[ R^m \to R^n \to M \to 0.  \]
\end{definition} 

The point of this definition is that $M$ is the quotient of a free module
$R^n$ by the ``relations'' given by the images of the vectors in $R^m$. 
Since $R^m$ is finitely generated, $M$ can be represented via finitely many
generators \emph{and} finitely many relations.

The reader should compare this with the definition of a \textbf{finitely
generated} module; there we only require an exact sequence
\[ R^n \to M \to 0.  \]

As usual, we establish the usual properties of finitely presented modules.

We start by showing that if a finitely presented module $M$ is generated by
finitely many elements, the ``module of relations'' among these generators is
finitely generated itself. The condition of finite presentation only states that
there is \emph{one} such set of generators such that the module of generators
is finitely generated. 
\begin{proposition} 
Suppose $M$ is finitely presented. Then if $R^m \twoheadrightarrow M$ is a
surjection, the kernel is finitely generated.
\end{proposition} 
\begin{proof} Let $K$ be the kernel of $R^m \twoheadrightarrow M$.
Consider an exact sequence
\[ F' \to F \to M \to 0 \]
where $F', F$ are finitely generated and free, which we can do as $M$ is
finitely presented.
Draw a commutative and exact diagram
\[ 
\xymatrix{
& F' \ar[r] &  F \ar[r] \ar@{-->}[d]  &  M \ar[r] \ar[d]  &  0 \\
0 \ar[r] &  K \ar[r] &  R^m \ar[r] &  M \ar[r] &  0
}
\]
The dotted arrow $F \to R^m$ exists as $F$ is projective. There is induced a
map $F' \to K$. 
We get a commutative and exact diagram
\[ 
\xymatrix{
& F' \ar[r]\ar[d]^f  &  F \ar[r] \ar[d]^g  &  M \ar[r] \ar[d]  &  0 \\
0 \ar[r] &  K \ar[r] &  R^m \ar[r] &  M \ar[r] &  0
},
\]
to which we can apply the snake lemma. There is an exact sequence
\[ 0 \to \coker(f) \to \coker(g) \to 0,  \]
which gives an isomorphism $\coker(f) \simeq \coker(g)$.
However, $\coker(g)$ is finitely generated, as a quotient of $R^m$.
Thus $\coker(f)$ is too.
Since we have an exact sequence
\[ 0 \to \im(f) \to K \to \coker(f) \to 0,  \]
and $\im(f)$ is finitely generated (as the image of a finitely generated
object, $F'$), we find by \rref{exact-fingen} that $\coker(f)$ is finitely generated.
\end{proof} 

\begin{proposition} \label{exact-finpres}
Given an exact sequence
\[ 0 \to M' \to M \to M'' \to 0,  \]
if $M', M''$ are finitely presented, so is $M$.
\end{proposition} 

In general, it is not true that if $M$ is finitely presented, then $M'$ and
$M''$ are. For instance, it is possible that a submodule of the free, finitely
generated module $R$ (i.e. an ideal), might fail to be finitely generated. We
shall see in \rref{noetherian} that this does not happen over a
\emph{noetherian} ring.

\begin{proof} 
Indeed, suppose we have  exact sequences
\[ F_1' \to F_0' \to M' \to 0   \]
and
\[ F_1'' \to F_0'' \to M'' \to 0  \]
where the $F$'s are finitely generated and free. 
We need to get a similar sequence for $M$. 
Let us stack these into a diagram
\[ \xymatrix{
& F_1' \ar[d] & & F_1'' \ar[d]  \\
& F_0' \ar[d]  & & F_0'' \ar[d] \\
0 \ar[r] &  M' \ar[r] &  M \ar[r] &  M'' \ar[r] &  0
}\]
However, now, using general facts about projective modules (\rref{}), we can
splice these presentations into a resolution
\[ F_1' \oplus F_1'' \to F_0' \oplus F_0'' \to M \to 0,  \]
which proves the assertion.
\end{proof} 


\begin{corollary} 
The (finite) direct sum of finitely presented modules is finitely presented.
\end{corollary} 
\begin{proof} 
Immediate from \rref{exact-finpres}
\end{proof} 

